# Video-to-Guide Pipeline

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](https://opensource.org/licenses/Apache-2.0)

A comprehensive toolkit for transforming instructional videos into professional documentation guides through automated transcription and AI-assisted guide generation.

## ğŸ¯ Overview

This pipeline automates the process of converting video content into structured, searchable documentation. Perfect for technical teams, content creators, and organizations looking to scale their documentation processes.

### **Complete Workflow:**
```
ğŸ“¹ Video Files â†’ ğŸµ Audio Extraction â†’ ğŸ“ Transcription â†’ ğŸ“š Guide Generation
```

## âœ¨ Features

- **ğŸµ High-Quality Audio Extraction** - FFmpeg-powered audio extraction optimized for speech recognition
- **ğŸ¤– Accurate Transcription** - OpenAI Whisper integration for precise speech-to-text conversion
- **ğŸ“‹ Template-Based Guide Generation** - Structured markdown guide creation with customizable templates
- **âš¡ Batch Processing** - Process multiple videos simultaneously
- **ğŸ”§ Configurable Pipeline** - YAML-based configuration for different use cases
- **ğŸ“Š Quality Assurance** - Built-in tools for reviewing and validating output
- **ğŸ³ Docker Support** - Containerized deployment for consistent environments

## ğŸ—ï¸ Architecture

### **Two-Stage Processing Pipeline**

This pipeline combines AI-powered transcription with intelligent text processing for **privacy, cost-efficiency, and offline capability**:

#### **Stage 1: Speech-to-Text (OpenAI Whisper - LOCAL)**
- **Technology**: OpenAI's open-source Whisper model
- **Execution**: Runs **locally on your machine** (no API calls)
- **Cost**: **$0.00** - completely free after initial setup
- **Privacy**: All audio processing happens offline
- **Models Available**: 
  - `tiny` - Fastest, basic accuracy (~39 MB)
  - `base` - Good balance of speed/accuracy (~74 MB) **[Default]**
  - `small` - Better accuracy (~244 MB)
  - `medium` - High accuracy (~769 MB)
  - `large` - Best accuracy (~1550 MB)

#### **Stage 2: Guide Generation (Template-Based Processing)**
- **Technology**: Template-based text processing using Jinja2
- **Method**: Intelligent pattern matching and structured formatting
- **Processing**: 
  - Extracts sections using natural language patterns
  - Identifies commands with shell syntax recognition
  - Finds URLs using regex validation
  - Structures content using template engines
- **Templates**: Customizable Markdown templates with structured formatting

### **Key Architectural Benefits**

| Aspect | Traditional API Approach | Our Architecture |
|--------|--------------------------|------------------|
| **Cost** | $0.10-$1.00+ per hour of audio | **$0.00** |
| **Privacy** | Audio sent to external servers | **100% local processing** |
| **Reliability** | Depends on internet/API availability | **Works offline** |
| **Speed** | Network latency + processing | **Local GPU/CPU only** |
| **Customization** | Limited by API constraints | **Full model control** |

### **Hardware Requirements**

- **CPU Processing**: Works on any modern CPU (slower)
- **GPU Acceleration**: CUDA-compatible GPU recommended for faster processing
- **RAM Requirements**:
  - `tiny/base` models: 2-4GB RAM
  - `small/medium` models: 4-8GB RAM  
  - `large` models: 8GB+ RAM
- **Storage**: 100MB - 2GB for model files (downloaded once)

### **No External Dependencies**

âœ… **No API keys required**  
âœ… **No internet connection needed** (after initial setup)  
âœ… **No external service accounts**  
âœ… **No usage limits or quotas**  
âœ… **Complete data privacy**  

## ğŸ“‹ Prerequisites

**System Requirements:**
- **Python 3.8+** - Check with `python3 --version`
- **FFmpeg** - Install with `sudo apt install ffmpeg` (Ubuntu) or `brew install ffmpeg` (macOS)
- **4GB+ RAM** - For Whisper model processing
- **Git** - For cloning the repository

**Optional:**
- **NVIDIA GPU** - For faster transcription (auto-detected)
- **8GB+ RAM** - For larger Whisper models

## ğŸš€ Quick Start

### One-Command Setup

```bash
# Clone and setup everything
git clone https://github.com/mik-tf/video-to-guide-pipeline.git
cd video-to-guide-pipeline
make setup
```

**That's it!** The Makefile handles:
- âœ… Python 3.8+ and FFmpeg verification
- âœ… Virtual environment creation
- âœ… PyTorch installation (GPU/CPU auto-detection)
- âœ… All dependencies installation
- âœ… Directory structure creation

### Process Videos

```bash
# Process all videos in ./videos directory
make process-videos

# Process a single video
make process-single VIDEO=path/to/video.mp4

# Run interactive demo
make demo
```

## ğŸ› ï¸ Makefile Commands

| Command | Description |
|---------|-------------|
| `make setup` | **Complete setup** - Creates venv, installs dependencies, sets up directories |
| `make process-videos` | **Process all videos** in `./videos/` directory |
| `make process-single VIDEO=path.mp4` | **Process single video** |
| `make demo` | **Interactive demo** with examples |
| `make check-deps` | **Check system dependencies** (Python, FFmpeg, GPU) |
| `make clean` | **Clean output files** (keeps venv) |
| `make clean-all` | **Clean everything** including venv |
| `make help` | **Show all commands** with examples |

### Usage Examples

```bash
# First time setup
make setup

# Activate the Python environment
source venv/bin/activate

# Add videos to ./videos/ directory
cp ~/my-tutorial.mp4 videos/

# Process all videos
make process-videos

# Process specific video
make process-single VIDEO=videos/my-tutorial.mp4

# Check what was generated
ls output/guides/
```

## ğŸ“ Project Structure

```
video-to-guide-pipeline/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ Makefile                     # ğŸ¯ Main entry point - all commands
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ .env.example                # Environment variables template
â”œâ”€â”€ docker-compose.yml          # Docker setup
â”œâ”€â”€ Dockerfile                  # Container definition
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml            # Default configuration
â”‚   â””â”€â”€ templates.yaml          # Template configurations
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio_extractor.py      # FFmpeg audio extraction
â”‚   â”œâ”€â”€ transcriber.py          # Whisper transcription
â”‚   â”œâ”€â”€ guide_generator.py      # Guide creation logic
â”‚   â”œâ”€â”€ template_engine.py      # Template processing
â”‚   â””â”€â”€ utils.py                # Common utilities
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process_videos.py       # Main processing script
â”‚   â”œâ”€â”€ setup_environment.sh    # Environment setup
â”‚   â””â”€â”€ validate_output.py      # Quality assurance
â”‚
â”œâ”€â”€ videos/                     # Input video files
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ audio/                  # Generated audio files
â”‚   â”œâ”€â”€ transcriptions/         # Text transcriptions
â”‚   â””â”€â”€ guides/                 # Final markdown guides
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ deployment_guide.md     # Deployment guide template
â”‚   â”œâ”€â”€ tutorial.md             # Tutorial template
â”‚   â””â”€â”€ api_documentation.md    # API docs template
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_audio_extractor.py
â”‚   â”œâ”€â”€ test_transcriber.py
â”‚   â””â”€â”€ test_guide_generator.py
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ usage.md                # Detailed usage guide
    â”œâ”€â”€ configuration.md        # Configuration options
    â”œâ”€â”€ templates.md            # Template customization
    â””â”€â”€ api.md                  # API documentation
```

## ğŸ”§ Configuration

The pipeline uses YAML configuration files for customization:

```yaml
# config/default.yaml
audio:
  sample_rate: 16000
  channels: 1
  format: "wav"

transcription:
  model: "base"  # tiny, base, small, medium, large
  language: "en"

guide_generation:
  template: "deployment_guide"
  include_timestamps: false
  max_section_length: 500
```

### **AI Model Configuration**

The pipeline's AI components are fully configurable:

```yaml
# Whisper Model Settings
transcription:
  model: "base"              # Model size: tiny, base, small, medium, large
  device: "cpu"              # Processing: cpu, cuda (GPU acceleration)
  language: "en"             # Target language (auto-detect if null)
  temperature: 0.0           # Sampling randomness (0.0 = deterministic)
  beam_size: 5               # Search breadth for accuracy
  fp16: false                # Half-precision (GPU only, faster)

# Guide Processing (Template-Based)
guide_generation:
  extract_commands: true     # Find shell commands automatically
  extract_urls: true         # Identify and validate URLs
  format_code_blocks: true   # Auto-format code snippets
  auto_generate_toc: true    # Create table of contents
```

**Model Selection Guide:**
- **Production**: Use `base` or `small` for best speed/accuracy balance
- **High Accuracy**: Use `medium` or `large` for technical content
- **Fast Processing**: Use `tiny` for quick drafts or testing
- **GPU Available**: Enable `fp16: true` and `device: "cuda"` for 2x speed boost

## ğŸ“š Usage Examples

### Process a Single Video
```bash
# Simple single video processing
make process-single VIDEO=videos/tutorial.mp4

# Or activate venv and use Python directly
source venv/bin/activate
python scripts/process_videos.py --input videos/tutorial.mp4
```

### Batch Process Multiple Videos
```bash
# Process all videos in ./videos directory
make process-videos

# Check system before processing
make check-deps
make process-videos
```

### Advanced Usage
```bash
# Interactive demo with examples
make demo

# Clean previous results and reprocess
make clean
make process-videos

# Check what's available
make help
```

## ğŸ¯ Use Cases

- **ğŸ“š Technical Documentation** - Convert deployment videos into step-by-step guides
- **ğŸ“ Training Materials** - Transform training videos into searchable documentation
- **ğŸ“‹ Process Documentation** - Document complex procedures from video demonstrations
- **ğŸ”„ Knowledge Transfer** - Preserve institutional knowledge from video content
- **ğŸ“ Content Repurposing** - Convert video content into multiple documentation formats

## ğŸ”§ Troubleshooting

### Common Issues

**âŒ "FFmpeg not found"**
```bash
# Ubuntu/Debian
sudo apt update && sudo apt install ffmpeg

# macOS
brew install ffmpeg

# Verify installation
ffmpeg -version
```

**âŒ "Python 3.8+ required"**
```bash
# Check current version
python3 --version

# Ubuntu: Install newer Python
sudo apt install python3.9 python3.9-venv
```

**âŒ "No videos found"**
```bash
# Check videos directory
ls -la videos/

# Copy videos to correct location
cp ~/Downloads/*.mp4 videos/
```

**âŒ "Virtual environment issues"**
```bash
# Clean and recreate
make clean-all
make setup
```

**âŒ "Out of memory during processing"**
```bash
# Use smaller Whisper model
# Edit config/default.yaml:
# model_name: "tiny"  # or "base"
```

### Getting Help

```bash
# Check system status
make check-deps

# Show all available commands
make help

# Run interactive demo
make demo
```

## ğŸ› ï¸ Advanced Features

### Custom Templates
Create your own guide templates by extending the base template system. See [docs/templates.md](docs/templates.md) for details.

### Quality Assurance
Built-in validation tools help ensure transcription accuracy and guide completeness:
```bash
python scripts/validate_output.py --check-transcription --check-guides
```

### Docker Deployment
Run the entire pipeline in a containerized environment:
```bash
# Build Docker image
make docker-build

# Run with Docker (processes videos/ directory)
make docker-run

# Or use docker-compose
docker-compose up --build
```

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI Whisper** - For excellent speech recognition capabilities
- **FFmpeg** - For robust audio/video processing